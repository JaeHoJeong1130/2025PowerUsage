import pandas as pd
import numpy as np
import xgboost as xgb
import catboost as cb
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import make_scorer
import optuna
import warnings
from tqdm import tqdm
import datetime
import random
import os
import joblib

# --- ê²½ë¡œ ì„¤ì • ë° ì‹œë“œ ê³ ì • ---
# ì‚¬ìš©ì í™˜ê²½ì— ë§ê²Œ ê²½ë¡œë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”.
path = "/home/jjh/Project/_data/dacon/electro/"
w_path = "/home/jjh/Project/_data/dacon/electro/wei/"
s_path = "/home/jjh/Project/_data/dacon/electro/sub/"
os.makedirs(w_path, exist_ok=True)
os.makedirs(s_path, exist_ok=True)
warnings.filterwarnings('ignore')

def seed_everything(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)

seed_everything(42)
print("ëœë¤ ì‹œë“œë¥¼ 42ë¡œ ê³ ì •í–ˆìŠµë‹ˆë‹¤.")

# --- SMAPE í‰ê°€ í•¨ìˆ˜ ---
def smape(true, pred):
    true = np.asarray(true)
    pred = np.asarray(pred)
    smape_val = np.mean((2 * np.abs(pred - true)) / (np.abs(true) + np.abs(pred) + 1e-8)) * 100
    return smape_val

smape_scorer = make_scorer(smape, greater_is_better=False)
print("SMAPE í‰ê°€ í•¨ìˆ˜ë¥¼ ì •ì˜í–ˆìŠµë‹ˆë‹¤.")


# --- ë°ì´í„° ë¡œë”© ë° í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ---
print("\n1 & 2. ë°ì´í„° ë¡œë”©, ì „ì²˜ë¦¬, ì‹¬í™” í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ìˆ˜í–‰...")
train_df = pd.read_csv(path + 'train.csv', encoding='utf-8')
test_df = pd.read_csv(path + 'test.csv', encoding='utf-8')
building_info_df = pd.read_csv(path + 'building_info.csv', encoding='utf-8')

building_info_df.replace('-', '0', inplace=True)
# ... (building_info_df ì „ì²˜ë¦¬ ì½”ë“œëŠ” ë™ì¼)
building_info_df['íƒœì–‘ê´‘ìš©ëŸ‰(kW)'] = building_info_df['íƒœì–‘ê´‘ìš©ëŸ‰(kW)'].astype(float)
building_info_df['ESSì €ì¥ìš©ëŸ‰(kWh)'] = building_info_df['ESSì €ì¥ìš©ëŸ‰(kWh)'].astype(float)
building_info_df['PCSìš©ëŸ‰(kW)'] = building_info_df['PCSìš©ëŸ‰(kW)'].astype(float)
building_info_df = pd.get_dummies(building_info_df, columns=['ê±´ë¬¼ìœ í˜•'], drop_first=True)


train_df = pd.merge(train_df, building_info_df, on='ê±´ë¬¼ë²ˆí˜¸')
test_df = pd.merge(test_df, building_info_df, on='ê±´ë¬¼ë²ˆí˜¸')

train_df.sort_values(['ê±´ë¬¼ë²ˆí˜¸', 'ì¼ì‹œ'], inplace=True)
test_df.sort_values(['ê±´ë¬¼ë²ˆí˜¸', 'ì¼ì‹œ'], inplace=True)


def feature_engineering(df):
    df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'])
    
    # 1. ì‹œê°„ í”¼ì²˜
    df['month'] = df['ì¼ì‹œ'].dt.month
    df['day'] = df['ì¼ì‹œ'].dt.day
    df['hour'] = df['ì¼ì‹œ'].dt.hour
    df['dayofweek'] = df['ì¼ì‹œ'].dt.dayofweek
    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)

    # 2. ì£¼ê¸°ì„± í”¼ì²˜
    df['sin_hour'] = np.sin(2 * np.pi * df['hour'] / 24)
    df['cos_hour'] = np.cos(2 * np.pi * df['hour'] / 24)
    df['sin_dayofweek'] = np.sin(2 * np.pi * df['dayofweek'] / 7)
    df['cos_dayofweek'] = np.cos(2 * np.pi * df['dayofweek'] / 7)
    
    # 3. íœ´ì¼ í”¼ì²˜
    holidays = [pd.to_datetime('2024-06-06'), pd.to_datetime('2024-08-15')]
    df['holiday'] = df['ì¼ì‹œ'].dt.date.isin([d.date() for d in holidays]).astype(int)
    df['before_holiday'] = df['holiday'].shift(-1).fillna(0)
    df['after_holiday'] = df['holiday'].shift(1).fillna(0)
    
    # 4. ê¸°ìƒ ê´€ë ¨ í”¼ì²˜
    df['discomfort_index'] = 9/5 * df['ê¸°ì˜¨(Â°C)'] - 0.55 * (1 - df['ìŠµë„(%)']/100) * (9/5 * df['ê¸°ì˜¨(Â°C)'] - 26) + 32
    
    # ê±´ë¬¼ë³„ ì´ë™ í†µê³„ëŸ‰
    for window in [3, 6, 12, 24]:
        df[f'temp_roll_mean_{window}h'] = df.groupby('ê±´ë¬¼ë²ˆí˜¸')['ê¸°ì˜¨(Â°C)'].transform(lambda x: x.rolling(window=window, min_periods=1).mean())
        df[f'temp_roll_std_{window}h'] = df.groupby('ê±´ë¬¼ë²ˆí˜¸')['ê¸°ì˜¨(Â°C)'].transform(lambda x: x.rolling(window=window, min_periods=1).std())
        df[f'humid_roll_mean_{window}h'] = df.groupby('ê±´ë¬¼ë²ˆí˜¸')['ìŠµë„(%)'].transform(lambda x: x.rolling(window=window, min_periods=1).mean())

    df.fillna(method='ffill', inplace=True)
    df.fillna(method='bfill', inplace=True)

    return df

train_df = feature_engineering(train_df)
test_df = feature_engineering(test_df)

# ê²€ì¦ ì„¸íŠ¸ ë¶„ë¦¬
train_df['ì¼ì‹œ'] = pd.to_datetime(train_df['ì¼ì‹œ'])
last_date = train_df['ì¼ì‹œ'].max()
validation_start_date = last_date - pd.Timedelta(days=4)
train_final_df = train_df[train_df['ì¼ì‹œ'] < validation_start_date].copy()
valid_final_df = train_df[train_df['ì¼ì‹œ'] >= validation_start_date].copy()
print(f"ìµœì¢… í›ˆë ¨ ë°ì´í„°: {train_final_df.shape}, ìµœì¢… ê²€ì¦ ë°ì´í„°: {valid_final_df.shape}")
print("ì „ì²˜ë¦¬ ì™„ë£Œ.")


# --- ëª¨ë¸ë§ ë° ì˜ˆì¸¡ ---
print("\n3. ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ ì‹œì‘...")
all_preds = []
N_TRIALS = 15 

for building_num in tqdm(range(1, 101), desc="ê±´ë¬¼ë³„ ëª¨ë¸ë§ ì§„í–‰"):
    # --- 1. ë°ì´í„° ì¤€ë¹„ ---
    train_building = train_final_df[train_final_df['ê±´ë¬¼ë²ˆí˜¸'] == building_num].copy()
    valid_building = valid_final_df[valid_final_df['ê±´ë¬¼ë²ˆí˜¸'] == building_num].copy()
    test_building = test_df[test_df['ê±´ë¬¼ë²ˆí˜¸'] == building_num].copy()

    features = [col for col in train_building.columns if col not in ['num_date_time', 'ì¼ì‹œ', 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)', 'ê±´ë¬¼ë²ˆí˜¸', 'ì¼ì¡°(hr)', 'ì¼ì‚¬(MJ/m2)']]
    
    X_train = train_building[features]
    y_train = train_building['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)']
    X_valid = valid_building[features]
    y_valid = valid_building['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'] # SMAPE ê³„ì‚°ì„ ìœ„í•´ ì›ë³¸ y_valid ìœ ì§€
    X_test = test_building[features]

    # <<<--- ğŸŒŸ y ë¡œê·¸ ë³€í™˜ --- START
    y_train_log = np.log1p(y_train)
    # <<<--- ğŸŒŸ y ë¡œê·¸ ë³€í™˜ --- END

    # --- 2. í”¼ì²˜ ìŠ¤ì¼€ì¼ë§ ---
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_valid_scaled = scaler.transform(X_valid)
    X_test_scaled = scaler.transform(X_test)
    joblib.dump(scaler, os.path.join(w_path, f'scaler_building_{building_num}.pkl'))

    # --- 3. Optuna íŠœë‹ (XGBoost) ---
    def xgb_objective(trial):
        params = {
            'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'n_estimators': 1000,
            'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),
            'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),
            'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.7, 0.8, 0.9, 1.0]),
            'subsample': trial.suggest_categorical('subsample', [0.7, 0.8, 0.9, 1.0]),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),
            'max_depth': trial.suggest_int('max_depth', 3, 9),
            'min_child_weight': trial.suggest_int('min_child_weight', 1, 9),
            'random_state': 42, 'n_jobs': -1,
        }
        model = xgb.XGBRegressor(**params)
        # <<<--- ğŸŒŸ ë¡œê·¸ ë³€í™˜ëœ yë¡œ í•™ìŠµ
        model.fit(X_train_scaled, y_train_log, eval_set=[(X_valid_scaled, np.log1p(y_valid))], early_stopping_rounds=50, verbose=False)
        preds_log = model.predict(X_valid_scaled)
        # <<<--- ğŸŒŸ ì˜ˆì¸¡ê°’ì„ ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë³µì›í•˜ì—¬ SMAPE ê³„ì‚°
        preds_original = np.expm1(preds_log)
        # ìŒìˆ˜ ì˜ˆì¸¡ ë°©ì§€
        preds_original[preds_original < 0] = 0
        return smape(y_valid, preds_original)

    study_xgb = optuna.create_study(direction='minimize')
    study_xgb.optimize(xgb_objective, n_trials=N_TRIALS, show_progress_bar=False)
    best_params_xgb = study_xgb.best_params
    
    # --- 4. Optuna íŠœë‹ (CatBoost) ---
    def cat_objective(trial):
        params = {
            'iterations': 1000, 'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),
            'depth': trial.suggest_int('depth', 3, 9),
            'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10.0),
            'bootstrap_type': 'Bernoulli', 'subsample': trial.suggest_float('subsample', 0.7, 1.0),
            'random_seed': 42, 'verbose': 0,
        }
        model = cb.CatBoostRegressor(**params)
        # <<<--- ğŸŒŸ ë¡œê·¸ ë³€í™˜ëœ yë¡œ í•™ìŠµ
        model.fit(X_train_scaled, y_train_log, eval_set=[(X_valid_scaled, np.log1p(y_valid))], early_stopping_rounds=50, verbose=False)
        preds_log = model.predict(X_valid_scaled)
        # <<<--- ğŸŒŸ ì˜ˆì¸¡ê°’ì„ ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë³µì›í•˜ì—¬ SMAPE ê³„ì‚°
        preds_original = np.expm1(preds_log)
        preds_original[preds_original < 0] = 0
        return smape(y_valid, preds_original)

    study_cat = optuna.create_study(direction='minimize')
    study_cat.optimize(cat_objective, n_trials=N_TRIALS, show_progress_bar=False)
    best_params_cat = study_cat.best_params

    # --- 5. ìµœì¢… ëª¨ë¸ í•™ìŠµ ---
    xgb_model = xgb.XGBRegressor(**best_params_xgb, n_estimators=2000, random_state=42, n_jobs=-1)
    xgb_model.fit(X_train_scaled, y_train_log, eval_set=[(X_valid_scaled, np.log1p(y_valid))], early_stopping_rounds=100, verbose=False)
    
    cat_model = cb.CatBoostRegressor(**best_params_cat, iterations=2000, random_seed=42, verbose=0)
    cat_model.fit(X_train_scaled, y_train_log, eval_set=[(X_valid_scaled, np.log1p(y_valid))], early_stopping_rounds=100, verbose=False)

    # --- 6. ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ê³„ì‚° ë° ì˜ˆì¸¡ ---
    xgb_valid_pred_log = xgb_model.predict(X_valid_scaled)
    cat_valid_pred_log = cat_model.predict(X_valid_scaled)
    
    # <<<--- ğŸŒŸ ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë³µì› í›„ SMAPE ê³„ì‚°
    xgb_valid_pred = np.expm1(xgb_valid_pred_log)
    cat_valid_pred = np.expm1(cat_valid_pred_log)
    xgb_valid_pred[xgb_valid_pred < 0] = 0
    cat_valid_pred[cat_valid_pred < 0] = 0

    smape_xgb = smape(y_valid, xgb_valid_pred)
    smape_cat = smape(y_valid, cat_valid_pred)

    weight_xgb = 1 / (smape_xgb + 1e-8)
    weight_cat = 1 / (smape_cat + 1e-8)
    total_weight = weight_xgb + weight_cat
    
    w1 = weight_xgb / total_weight
    w2 = weight_cat / total_weight

    # <<<--- ğŸŒŸ ìµœì¢… ì˜ˆì¸¡ê°’ë„ ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë³µì›
    xgb_pred_log = xgb_model.predict(X_test_scaled)
    cat_pred_log = cat_model.predict(X_test_scaled)
    
    xgb_pred = np.expm1(xgb_pred_log)
    cat_pred = np.expm1(cat_pred_log)

    ensemble_pred = w1 * xgb_pred + w2 * cat_pred
    ensemble_pred[ensemble_pred < 0] = 0

    # --- 7. ê²°ê³¼ ì €ì¥ ---
    building_preds = pd.DataFrame({'num_date_time': test_building['num_date_time'], 'answer': ensemble_pred})
    all_preds.append(building_preds)
    
    xgb_model.save_model(os.path.join(w_path, f'xgb_model_building_{building_num}.json'))
    cat_model.save_model(os.path.join(w_path, f'catboost_model_building_{building_num}.cbm'))

print("ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ ì™„ë£Œ. ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...")

# --- ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„± ---
timestamp = datetime.datetime.now().strftime("%m%d%H%M%S")
file_name = f"submission_{timestamp}.csv"
file_path = os.path.join(s_path, file_name)

final_submission = pd.concat(all_preds, ignore_index=True)
final_submission.to_csv(file_path, index=False)

print(f"\nì œì¶œ íŒŒì¼ì´ '{file_path}'ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
print("í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ.")