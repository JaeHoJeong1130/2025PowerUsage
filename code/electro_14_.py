import pandas as pd
import numpy as np
import xgboost as xgb
import catboost as cb
import optuna
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
import warnings
from tqdm import tqdm
import datetime
import random
import os
import joblib

# --- ê²½ë¡œ ì„¤ì • ë° ì‹œë“œ ê³ ì • ---
# !!! ì‚¬ìš© ì „ ê²½ë¡œë¥¼ ì‹¤ì œ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •í•´ì£¼ì„¸ìš” !!!
# í˜„ì¬ íŒŒì¼ë“¤ì´ ì—…ë¡œë“œëœ ê°€ìƒ í™˜ê²½ì„ ê²½ë¡œë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
path = "/home/jjh/Project/_data/dacon/electro/"
w_path = "/home/jjh/Project/_data/dacon/electro/wei/"
s_path = "/home/jjh/Project/_data/dacon/electro/sub/"

os.makedirs(w_path, exist_ok=True)
os.makedirs(s_path, exist_ok=True)
warnings.filterwarnings('ignore')

def seed_everything(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)

seed_everything(42)
print("âœ… ëœë¤ ì‹œë“œë¥¼ 42ë¡œ ê³ ì •í–ˆìŠµë‹ˆë‹¤.")

# SMAPE í‰ê°€ ì‚°ì‹ ì •ì˜
def smape(true, pred):
    epsilon = 1e-10
    return np.mean((2 * np.abs(pred - true)) / (np.abs(true) + np.abs(pred) + epsilon)) * 100

# --- 1. ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ (ì œê³µëœ ì½”ë“œ í™œìš©) ---
print("\n[ë‹¨ê³„ 1/5] ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
try:
    train_df = pd.read_csv(path + 'train.csv', encoding='utf-8')
    test_df = pd.read_csv(path + 'test.csv', encoding='utf-8')
    building_info_df = pd.read_csv(path + 'building_info.csv', encoding='utf-8')
    sample_submission_df = pd.read_csv(path + 'sample_submission.csv', encoding='utf-8')
except FileNotFoundError:
    print("ì˜¤ë¥˜: CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'path' ë³€ìˆ˜ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    # í•„ìš”í•œ ê²½ìš° ì—¬ê¸°ì„œ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ì„ ì¤‘ë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    exit()


building_info_df.replace('-', '0', inplace=True)
building_info_df['íƒœì–‘ê´‘ìš©ëŸ‰(kW)'] = building_info_df['íƒœì–‘ê´‘ìš©ëŸ‰(kW)'].astype(float)
building_info_df['ESSì €ì¥ìš©ëŸ‰(kWh)'] = building_info_df['ESSì €ì¥ìš©ëŸ‰(kWh)'].astype(float)
building_info_df['PCSìš©ëŸ‰(kW)'] = building_info_df['PCSìš©ëŸ‰(kW)'].astype(float)
building_info_df = pd.get_dummies(building_info_df, columns=['ê±´ë¬¼ìœ í˜•'], drop_first=True, prefix='ê±´ë¬¼ìœ í˜•')

# --- 2. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (ì œê³µëœ ì½”ë“œ í™œìš©) ---
print("[ë‹¨ê³„ 2/5] í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤...")
building_groups_str = """
87 80;50;30 17 63 18 31 53 49 51 52 43 48 64 29 76 78 40 60;54 84;56 38 36;39 59 92 73 16 15;9 10;71;46 95 94 93;98;75 58 91 19;77;90 89;72;82 55;41 88 68 83 12 11 13;66;22 23 21;97;34 33 37 1 27 3 2 5 6 7 4 96 67 86 35 47;85;57;8;81 61 74;28 14 69;24;44 100 26 45 70 20;62 25;32 42 79 65 99
"""
groups = building_groups_str.strip().replace('\n', ';').split(';')
building_group_map = {}
for i, group in enumerate(groups):
    buildings = map(int, group.split())
    for building_num in buildings:
        building_group_map[building_num] = i
building_info_df['building_group'] = building_info_df['ê±´ë¬¼ë²ˆí˜¸'].map(building_group_map)

train_df = pd.merge(train_df, building_info_df, on='ê±´ë¬¼ë²ˆí˜¸')
test_df = pd.merge(test_df, building_info_df, on='ê±´ë¬¼ë²ˆí˜¸')

def feature_engineering(df):
    df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'], format='%Y%m%d %H')
    df['month'] = df['ì¼ì‹œ'].dt.month
    df['day'] = df['ì¼ì‹œ'].dt.day
    df['hour'] = df['ì¼ì‹œ'].dt.hour
    df['dayofweek'] = df['ì¼ì‹œ'].dt.dayofweek # 0:ì›”, 1:í™”, 2:ìˆ˜, 3:ëª©, 4:ê¸ˆ, 5:í† , 6:ì¼
    df['MMDDHH'] = df['month'].astype(str).str.zfill(2) + df['day'].astype(str).str.zfill(2) + df['hour'].astype(str).str.zfill(2)
    df['MMDDHH'] = df['MMDDHH'].astype(int)
    holidays = [pd.to_datetime('2024-06-06'), pd.to_datetime('2024-08-15')]
    df['holiday'] = df['ì¼ì‹œ'].dt.date.isin([d.date() for d in holidays]).astype(int)
    df['discomfort_index'] = 9/5 * df['ê¸°ì˜¨(Â°C)'] - 0.55 * (1 - df['ìŠµë„(%)']/100) * (9/5 * df['ê¸°ì˜¨(Â°C)'] - 26) + 32
    df['sin_hour'] = np.sin(2 * np.pi * df['hour'] / 24)
    df['cos_hour'] = np.cos(2 * np.pi * df['hour'] / 24)
    df['is_weekend'] = (df['ì¼ì‹œ'].dt.dayofweek >= 5).astype(int)
    df['day_of_year'] = df['ì¼ì‹œ'].dt.dayofyear
    return df

train_df = feature_engineering(train_df)
test_df = feature_engineering(test_df)
print("âœ… í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ!")

# --- 3. ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ ---
print("[ë‹¨ê³„ 3/5] ê±´ë¬¼ë³„ ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

# Public Score ê¸°ê°„(8/25~27)ì€ ì¼,ì›”,í™” ìš”ì¼
target_days = [6, 0, 1] # 6:ì¼, 0:ì›”, 1:í™”
train_filtered = train_df[train_df['dayofweek'].isin(target_days)]

features = [col for col in test_df.columns if col not in ['num_date_time', 'ê±´ë¬¼ë²ˆí˜¸', 'ì¼ì‹œ']]
all_building_preds = []

for building_num in tqdm(range(1, 101), desc="ê±´ë¬¼ë³„ ì˜ˆì¸¡ ì§„í–‰ ì¤‘"):
    
    # --- ë°ì´í„° ì¤€ë¹„ ---
    train_building = train_filtered[train_filtered['ê±´ë¬¼ë²ˆí˜¸'] == building_num].copy()
    test_building = test_df[test_df['ê±´ë¬¼ë²ˆí˜¸'] == building_num].copy()

    X_train = train_building[features]
    y_train = train_building['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)']
    X_test = test_building[features]
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ì ìš©
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    X_train_scaled = pd.DataFrame(X_train_scaled, columns=features)
    X_test_scaled = pd.DataFrame(X_test_scaled, columns=features)

    # --- Optunaë¥¼ ì´ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ---
    def objective(trial, model_name, X, y):
        if model_name == 'xgb':
            params = {
                'objective': 'reg:squarederror',
                'eval_metric': 'rmse',
                'n_estimators': trial.suggest_int('n_estimators', 500, 2000),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                'max_depth': trial.suggest_int('max_depth', 3, 10),
                'subsample': trial.suggest_float('subsample', 0.6, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
                'random_state': 42
            }
            model_class = xgb.XGBRegressor
        else: # cat
            params = {
                'iterations': trial.suggest_int('iterations', 500, 2000),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                'depth': trial.suggest_int('depth', 3, 10),
                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 10.0, log=True),
                'random_state': 42,
                'verbose': 0
            }
            model_class = cb.CatBoostRegressor

        kf = KFold(n_splits=5, shuffle=True, random_state=42)
        scores = []
        for train_idx, val_idx in kf.split(X):
            X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]
            y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]
            
            model = model_class(**params, early_stopping_rounds=50,)
            model.fit(X_train_fold, y_train_fold, eval_set=[(X_val_fold, y_val_fold)], verbose=False)
            preds = model.predict(X_val_fold)
            scores.append(smape(y_val_fold, preds))
        
        return np.mean(scores)

    # XGBoost íŠœë‹
    study_xgb = optuna.create_study(direction='minimize')
    study_xgb.optimize(lambda trial: objective(trial, 'xgb', X_train_scaled, y_train), n_trials=20, n_jobs=-1)
    best_params_xgb = study_xgb.best_params
    
    # CatBoost íŠœë‹
    study_cat = optuna.create_study(direction='minimize')
    study_cat.optimize(lambda trial: objective(trial, 'cat', X_train_scaled, y_train), n_trials=20, n_jobs=-1)
    best_params_cat = study_cat.best_params

    # --- K-Fold êµì°¨ê²€ì¦ ê¸°ë°˜ ëª¨ë¸ í•™ìŠµ ë° ì•™ìƒë¸” ---
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    
    oof_preds_xgb = np.zeros(len(X_train_scaled))
    test_preds_xgb = np.zeros(len(X_test_scaled))
    oof_preds_cat = np.zeros(len(X_train_scaled))
    test_preds_cat = np.zeros(len(X_test_scaled))

    for train_idx, val_idx in kf.split(X_train_scaled):
        X_train_fold, X_val_fold = X_train_scaled.iloc[train_idx], X_train_scaled.iloc[val_idx]
        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]

        # XGBoost
        model_xgb = xgb.XGBRegressor(**best_params_xgb, random_state=42)
        model_xgb.fit(X_train_fold, y_train_fold)
        oof_preds_xgb[val_idx] = model_xgb.predict(X_val_fold)
        test_preds_xgb += model_xgb.predict(X_test_scaled) / kf.n_splits

        # CatBoost
        model_cat = cb.CatBoostRegressor(**best_params_cat, random_state=42, verbose=0)
        model_cat.fit(X_train_fold, y_train_fold)
        oof_preds_cat[val_idx] = model_cat.predict(X_val_fold)
        test_preds_cat += model_cat.predict(X_test_scaled) / kf.n_splits

    # SMAPE ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
    smape_xgb = smape(y_train, oof_preds_xgb)
    smape_cat = smape(y_train, oof_preds_cat)
    
    w_cat = smape_xgb / (smape_xgb + smape_cat)
    w_xgb = smape_cat / (smape_xgb + smape_cat)
    
    # ê°€ì¤‘ì¹˜ ì €ì¥
    weights = {'w_xgb': w_xgb, 'w_cat': w_cat, 'smape_xgb': smape_xgb, 'smape_cat': smape_cat}
    joblib.dump(weights, f'{w_path}building_{building_num}_weights.pkl')

    # ìµœì¢… ì˜ˆì¸¡
    final_preds = w_xgb * test_preds_xgb + w_cat * test_preds_cat
    final_preds[final_preds < 0] = 0 # ì „ë ¥ëŸ‰ì€ ìŒìˆ˜ê°€ ë  ìˆ˜ ì—†ìŒ

    building_pred_df = pd.DataFrame({'num_date_time': test_building['num_date_time'], 'answer': final_preds})
    all_building_preds.append(building_pred_df)

print("âœ… ëª¨ë“  ê±´ë¬¼ì˜ ì˜ˆì¸¡ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

# --- 4. ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„± ---
print("[ë‹¨ê³„ 4/5] ìµœì¢… ì œì¶œ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤...")
final_submission = pd.concat(all_building_preds, ignore_index=True)
final_submission = final_submission.sort_values(by=['num_date_time']).reset_index(drop=True)

# --- 5. íŒŒì¼ ì €ì¥ ---
final_submission_path = f"{s_path}final_submission.csv"
final_submission.to_csv(final_submission_path, index=False)

print(f"âœ… ìµœì¢… ì œì¶œ íŒŒì¼ì´ '{final_submission_path}' ê²½ë¡œì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
print("[ë‹¨ê³„ 5/5] ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ğŸ‰")