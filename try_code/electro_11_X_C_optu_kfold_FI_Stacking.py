import pandas as pd
import numpy as np
import xgboost as xgb
import catboost as cb
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import make_scorer
from sklearn.linear_model import Ridge # <<<--- ğŸŒŸ ìŠ¤íƒœí‚¹ì„ ìœ„í•œ Ridge ëª¨ë¸ ì¶”ê°€
import optuna
import warnings
from tqdm import tqdm
import datetime
import random
import os
import joblib

# --- ê²½ë¡œ ì„¤ì • ë° ì‹œë“œ ê³ ì • ---
path = "/home/jjh/Project/_data/dacon/electro/"
w_path = "/home/jjh/Project/_data/dacon/electro/wei/"
s_path = "/home/jjh/Project/_data/dacon/electro/sub/"
os.makedirs(w_path, exist_ok=True)
os.makedirs(s_path, exist_ok=True)
warnings.filterwarnings('ignore')

def seed_everything(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)

seed_everything(42)
print("ëœë¤ ì‹œë“œë¥¼ 42ë¡œ ê³ ì •í–ˆìŠµë‹ˆë‹¤.")

# --- SMAPE í‰ê°€ í•¨ìˆ˜ ---
def smape(true, pred):
    true = np.asarray(true)
    pred = np.asarray(pred)
    smape_val = np.mean((2 * np.abs(pred - true)) / (np.abs(true) + np.abs(pred) + 1e-8)) * 100
    return smape_val

smape_scorer = make_scorer(smape, greater_is_better=False)
print("SMAPE í‰ê°€ í•¨ìˆ˜ë¥¼ ì •ì˜í–ˆìŠµë‹ˆë‹¤.")

# --- ë°ì´í„° ë¡œë”© ë° í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ---
print("\n1 & 2. ë°ì´í„° ë¡œë”©, ì „ì²˜ë¦¬, ì‹¬í™” í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ìˆ˜í–‰...")
train_df = pd.read_csv(path + 'train.csv', encoding='utf-8')
test_df = pd.read_csv(path + 'test.csv', encoding='utf-8')
building_info_df = pd.read_csv(path + 'building_info.csv', encoding='utf-8')

building_info_df.replace('-', '0', inplace=True)
building_info_df['íƒœì–‘ê´‘ìš©ëŸ‰(kW)'] = building_info_df['íƒœì–‘ê´‘ìš©ëŸ‰(kW)'].astype(float)
building_info_df['ESSì €ì¥ìš©ëŸ‰(kWh)'] = building_info_df['ESSì €ì¥ìš©ëŸ‰(kWh)'].astype(float)
building_info_df['PCSìš©ëŸ‰(kW)'] = building_info_df['PCSìš©ëŸ‰(kW)'].astype(float)
building_info_df = pd.get_dummies(building_info_df, columns=['ê±´ë¬¼ìœ í˜•'], drop_first=True)

train_df = pd.merge(train_df, building_info_df, on='ê±´ë¬¼ë²ˆí˜¸')
test_df = pd.merge(test_df, building_info_df, on='ê±´ë¬¼ë²ˆí˜¸')

train_df.sort_values(['ê±´ë¬¼ë²ˆí˜¸', 'ì¼ì‹œ'], inplace=True)
test_df.sort_values(['ê±´ë¬¼ë²ˆí˜¸', 'ì¼ì‹œ'], inplace=True)

def feature_engineering(df):
    df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'])
    df['month'] = df['ì¼ì‹œ'].dt.month
    df['day'] = df['ì¼ì‹œ'].dt.day
    df['hour'] = df['ì¼ì‹œ'].dt.hour
    df['dayofweek'] = df['ì¼ì‹œ'].dt.dayofweek
    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)
    df['sin_hour'] = np.sin(2 * np.pi * df['hour'] / 24)
    df['cos_hour'] = np.cos(2 * np.pi * df['hour'] / 24)
    df['sin_dayofweek'] = np.sin(2 * np.pi * df['dayofweek'] / 7)
    df['cos_dayofweek'] = np.cos(2 * np.pi * df['dayofweek'] / 7)
    holidays = [pd.to_datetime('2024-06-06'), pd.to_datetime('2024-08-15')]
    df['holiday'] = df['ì¼ì‹œ'].dt.date.isin([d.date() for d in holidays]).astype(int)
    df['before_holiday'] = df['holiday'].shift(-1).fillna(0)
    df['after_holiday'] = df['holiday'].shift(1).fillna(0)
    df['discomfort_index'] = 9/5 * df['ê¸°ì˜¨(Â°C)'] - 0.55 * (1 - df['ìŠµë„(%)']/100) * (9/5 * df['ê¸°ì˜¨(Â°C)'] - 26) + 32
    for window in [3, 6, 12, 24]:
        df[f'temp_roll_mean_{window}h'] = df.groupby('ê±´ë¬¼ë²ˆí˜¸')['ê¸°ì˜¨(Â°C)'].transform(lambda x: x.rolling(window=window, min_periods=1).mean())
        df[f'temp_roll_std_{window}h'] = df.groupby('ê±´ë¬¼ë²ˆí˜¸')['ê¸°ì˜¨(Â°C)'].transform(lambda x: x.rolling(window=window, min_periods=1).std())
        df[f'humid_roll_mean_{window}h'] = df.groupby('ê±´ë¬¼ë²ˆí˜¸')['ìŠµë„(%)'].transform(lambda x: x.rolling(window=window, min_periods=1).mean())
    df.fillna(method='ffill', inplace=True)
    df.fillna(method='bfill', inplace=True)
    return df

train_df = feature_engineering(train_df)
test_df = feature_engineering(test_df)

train_df['ì¼ì‹œ'] = pd.to_datetime(train_df['ì¼ì‹œ'])
last_date = train_df['ì¼ì‹œ'].max()
validation_start_date = last_date - pd.Timedelta(days=4)
train_final_df = train_df[train_df['ì¼ì‹œ'] < validation_start_date].copy()
valid_final_df = train_df[train_df['ì¼ì‹œ'] >= validation_start_date].copy()
print(f"ìµœì¢… í›ˆë ¨ ë°ì´í„°: {train_final_df.shape}, ìµœì¢… ê²€ì¦ ë°ì´í„°: {valid_final_df.shape}")
print("ì „ì²˜ë¦¬ ì™„ë£Œ.")


# --- ëª¨ë¸ë§ ë° ì˜ˆì¸¡ ---
print("\n3. ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ ì‹œì‘...")
all_preds = []
N_TRIALS = 15 

for building_num in tqdm(range(1, 101), desc="ê±´ë¬¼ë³„ ëª¨ë¸ë§ ì§„í–‰"):
    # --- 1. ë°ì´í„° ì¤€ë¹„ ---
    train_building = train_final_df[train_final_df['ê±´ë¬¼ë²ˆí˜¸'] == building_num].copy()
    valid_building = valid_final_df[valid_final_df['ê±´ë¬¼ë²ˆí˜¸'] == building_num].copy()
    test_building = test_df[test_df['ê±´ë¬¼ë²ˆí˜¸'] == building_num].copy()

    initial_features = [col for col in train_building.columns if col not in ['num_date_time', 'ì¼ì‹œ', 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)', 'ê±´ë¬¼ë²ˆí˜¸', 'ì¼ì¡°(hr)', 'ì¼ì‚¬(MJ/m2)']]
    
    X_train_initial = train_building[initial_features]
    y_train = train_building['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)']
    X_valid_initial = valid_building[initial_features]
    y_valid = valid_building['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)']
    X_test_initial = test_building[initial_features]

    y_train_log = np.log1p(y_train)

    # --- 2. ì´ˆê¸° í”¼ì²˜ ìŠ¤ì¼€ì¼ë§ (Optuna ë° í”¼ì²˜ ì„ íƒìš©) ---
    scaler_initial = StandardScaler()
    X_train_scaled_initial = scaler_initial.fit_transform(X_train_initial)
    X_valid_scaled_initial = scaler_initial.transform(X_valid_initial)
    
    # --- 3. Optuna íŠœë‹ (XGBoost) ---
    def xgb_objective(trial):
        params = {
            'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'n_estimators': 1000,
            'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),
            'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),
            'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.7, 0.8, 0.9, 1.0]),
            'subsample': trial.suggest_categorical('subsample', [0.7, 0.8, 0.9, 1.0]),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),
            'max_depth': trial.suggest_int('max_depth', 3, 9),
            'min_child_weight': trial.suggest_int('min_child_weight', 1, 9),
            'random_state': 42, 'n_jobs': -1,
        }
        model = xgb.XGBRegressor(**params)
        model.fit(X_train_scaled_initial, y_train_log, eval_set=[(X_valid_scaled_initial, np.log1p(y_valid))], early_stopping_rounds=50, verbose=False)
        preds_log = model.predict(X_valid_scaled_initial)
        preds_original = np.expm1(preds_log)
        preds_original[preds_original < 0] = 0
        return smape(y_valid, preds_original)

    study_xgb = optuna.create_study(direction='minimize')
    study_xgb.optimize(xgb_objective, n_trials=N_TRIALS, show_progress_bar=False)
    best_params_xgb = study_xgb.best_params
    
    # --- 4. Optuna íŠœë‹ (CatBoost) ---
    def cat_objective(trial):
        params = {
            'iterations': 1000, 'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),
            'depth': trial.suggest_int('depth', 3, 9),
            'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10.0),
            'bootstrap_type': 'Bernoulli', 'subsample': trial.suggest_float('subsample', 0.7, 1.0),
            'random_seed': 42, 'verbose': 0,
        }
        model = cb.CatBoostRegressor(**params)
        model.fit(X_train_scaled_initial, y_train_log, eval_set=[(X_valid_scaled_initial, np.log1p(y_valid))], early_stopping_rounds=50, verbose=False)
        preds_log = model.predict(X_valid_scaled_initial)
        preds_original = np.expm1(preds_log)
        preds_original[preds_original < 0] = 0
        return smape(y_valid, preds_original)

    study_cat = optuna.create_study(direction='minimize')
    study_cat.optimize(cat_objective, n_trials=N_TRIALS, show_progress_bar=False)
    best_params_cat = study_cat.best_params

    # <<<--- ğŸŒŸ 5. í”¼ì²˜ ì„ íƒ ë¡œì§ ì¶”ê°€ --- START
    temp_xgb = xgb.XGBRegressor(**best_params_xgb, random_state=42).fit(X_train_scaled_initial, y_train_log)
    temp_cat = cb.CatBoostRegressor(**best_params_cat, random_seed=42, verbose=0).fit(X_train_scaled_initial, y_train_log)

    fi_df = pd.DataFrame({
        'feature': initial_features,
        'xgb_imp': temp_xgb.feature_importances_,
        'cat_imp': temp_cat.feature_importances_
    })
    fi_df['xgb_imp_norm'] = (fi_df['xgb_imp'] - fi_df['xgb_imp'].min()) / (fi_df['xgb_imp'].max() - fi_df['xgb_imp'].min() + 1e-8)
    fi_df['cat_imp_norm'] = (fi_df['cat_imp'] - fi_df['cat_imp'].min()) / (fi_df['cat_imp'].max() - fi_df['cat_imp'].min() + 1e-8)
    fi_df['combined_imp'] = (fi_df['xgb_imp_norm'] + fi_df['cat_imp_norm']) / 2
    
    # ì¤‘ìš”ë„ê°€ í‰ê·  ì´ìƒì¸ í”¼ì²˜ë“¤ë§Œ ì„ íƒ
    imp_threshold = fi_df['combined_imp'].mean()
    selected_features = fi_df[fi_df['combined_imp'] > imp_threshold]['feature'].tolist()
    if not selected_features: selected_features = initial_features

    # ì„ íƒëœ í”¼ì²˜ë¡œ ë°ì´í„°ì…‹ ì¬êµ¬ì„±
    X_train = train_building[selected_features]
    X_valid = valid_building[selected_features]
    X_test = test_building[selected_features]
    
    # ì„ íƒëœ í”¼ì²˜ì— ëŒ€í•´ ìŠ¤ì¼€ì¼ëŸ¬ ì¬í•™ìŠµ ë° ë³€í™˜
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_valid_scaled = scaler.transform(X_valid)
    X_test_scaled = scaler.transform(X_test)
    joblib.dump(scaler, os.path.join(w_path, f'scaler_building_{building_num}.pkl'))
    # <<<--- ğŸŒŸ í”¼ì²˜ ì„ íƒ ë¡œì§ ì¶”ê°€ --- END
    
    # --- 6. ìµœì¢… ëª¨ë¸ í•™ìŠµ (ì„ íƒëœ í”¼ì²˜ ì‚¬ìš©) ---
    xgb_model = xgb.XGBRegressor(**best_params_xgb, n_estimators=2000, random_state=42, n_jobs=-1)
    xgb_model.fit(X_train_scaled, y_train_log, eval_set=[(X_valid_scaled, np.log1p(y_valid))], early_stopping_rounds=100, verbose=False)
    
    cat_model = cb.CatBoostRegressor(**best_params_cat, iterations=2000, random_seed=42, verbose=0)
    cat_model.fit(X_train_scaled, y_train_log, eval_set=[(X_valid_scaled, np.log1p(y_valid))], early_stopping_rounds=100, verbose=False)

    # <<<--- ğŸŒŸ 7. ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ë° ì˜ˆì¸¡ --- START
    # Base Modelë“¤ì˜ ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡ (Meta Model í•™ìŠµìš©)
    xgb_valid_pred_log = xgb_model.predict(X_valid_scaled)
    cat_valid_pred_log = cat_model.predict(X_valid_scaled)
    xgb_valid_pred = np.expm1(xgb_valid_pred_log); xgb_valid_pred[xgb_valid_pred < 0] = 0
    cat_valid_pred = np.expm1(cat_valid_pred_log); cat_valid_pred[cat_valid_pred < 0] = 0
    
    # Meta Model í•™ìŠµ ë°ì´í„° ìƒì„±
    X_meta_train = np.c_[xgb_valid_pred, cat_valid_pred]
    
    # Meta Model í•™ìŠµ
    meta_model = Ridge(random_state=42)
    meta_model.fit(X_meta_train, y_valid)

    # Base Modelë“¤ì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡
    xgb_test_pred_log = xgb_model.predict(X_test_scaled)
    cat_test_pred_log = cat_model.predict(X_test_scaled)
    xgb_test_pred = np.expm1(xgb_test_pred_log); xgb_test_pred[xgb_test_pred < 0] = 0
    cat_test_pred = np.expm1(cat_test_pred_log); cat_test_pred[cat_test_pred < 0] = 0
    
    # Meta Model ì˜ˆì¸¡ ë°ì´í„° ìƒì„±
    X_meta_test = np.c_[xgb_test_pred, cat_test_pred]

    # ìµœì¢… ìŠ¤íƒœí‚¹ ì˜ˆì¸¡
    stacked_pred = meta_model.predict(X_meta_test)
    stacked_pred[stacked_pred < 0] = 0
    # <<<--- ğŸŒŸ ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ë° ì˜ˆì¸¡ --- END

    # --- 8. ê²°ê³¼ ì €ì¥ ---
    building_preds = pd.DataFrame({'num_date_time': test_building['num_date_time'], 'answer': stacked_pred})
    all_preds.append(building_preds)
    
    xgb_model.save_model(os.path.join(w_path, f'xgb_model_building_{building_num}.json'))
    cat_model.save_model(os.path.join(w_path, f'catboost_model_building_{building_num}.cbm'))
    joblib.dump(meta_model, os.path.join(w_path, f'meta_model_building_{building_num}.pkl')) # Meta Model ì €ì¥

print("ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ ì™„ë£Œ. ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...")

# --- ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„± ---
timestamp = datetime.datetime.now().strftime("%m%d%H%M%S")
file_name = f"submission_{timestamp}.csv"
file_path = os.path.join(s_path, file_name)

final_submission = pd.concat(all_preds, ignore_index=True)
final_submission.to_csv(file_path, index=False)

print(f"\nì œì¶œ íŒŒì¼ì´ '{file_path}'ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
print("í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ.")